{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwmm8IfoNY5l"
   },
   "source": [
    "# Smart Home Dataset – End-to-end Machine Learning project \n",
    "\n",
    "**IoT Course**  &nbsp;&nbsp;&nbsp; **Last modification:** 03/12/2021\n",
    "\n",
    "**Name:** \n",
    "\n",
    "## Content\n",
    "\n",
    "* <a href=\"#Section1\">Section 1: Exploratory data analysis</a>\n",
    "\n",
    "> * 1. <a href=\"#1.\">Setup - Library import</a> \n",
    "    * 1.1. <a href=\"#LibInf\">Library information</a> \n",
    "> * 2. <a href=\"#2.\">Data collection</a> \n",
    "    * 2.1 <a href=\"#2.2\">Accessing Google Drive by mounting it locally</a> \n",
    "> * 3. <a href=\"#3.\">Discovering and visualizing the data to gain insight</a> \n",
    "    * 3.1 <a href=\"#3.1\">Preprocessing</a> \n",
    "        * 3.1.1 <a href=\"#3.1.1\">Data exploration</a>\n",
    "            * Quick Look at the Data Structure\n",
    "            * How many and what type of attributes do we have?\n",
    "            * Finding out the categories within the object-type data \n",
    "            * Summary of each nummerical attribute\n",
    "            * Histogram for each numerical attribute\n",
    "        * 3.1.2 <a href=\"#3.1.2\">Data cleansing</a>\n",
    "            * Searching for NaNs\n",
    "        * 3.1.3 <a href=\"#3.1.3\">Feature engineering</a>\n",
    "            * Working with Time Series\n",
    "            * Looking for correlation\n",
    "        * 3.1.4 <a href=\"#3.1.4\">Visualization / Outputs</a>\n",
    "> * 4. <a href=\"#4.\">Exercise 1: Exploratory data analysis</a> \n",
    "        * 4.1 <a href=\"#4.1\">Visualise consumption by season of the year</a> \n",
    "        * 4.1.1 <a href=\"#4.1.1\">In which month within each season is the most energy consumed? </a> \n",
    "        * 4.1.2 <a href=\"#4.1.2\">Which appliances consume the most energy within that period?</a> \n",
    "        * 4.1.3 <a href=\"#4.1.3\">Is there any relationship between weather conditions and the highest energy consumption in those peak periods?</a> \n",
    "        * 4.1.4 <a href=\"#4.1.4\">In which weather conditions was the most energy consumed?</a> \n",
    "        \n",
    "* <a href=\"#Section2\">Section 2: Machine Learning and Deep Learning models</a>\n",
    "\n",
    "> * 5. <a href=\"#5.\">Data analysis - Different approaches</a> \n",
    "    * 5.1 <a href=\"#5.1\">Non machine learning approach</a> \n",
    "        * Moving Average\n",
    "        * Anomaly detection\n",
    "    * 5.2 <a href=\"#5.2\">Machine learning approach</a> \n",
    "        * 5.2.1 <a href=\"#5.2.1\">Simple ML Algorithm implementation using Scikit-Learn</a> \n",
    "            * Data preparation \n",
    "            * Linear Regression\n",
    "            * Random Forest Regressor\n",
    "        * 5.2.2 <a href=\"#5.2.2\">Evaluation of an ML algorithm (Regression)</a> \n",
    "        * 5.2.3 <a href=\"#5.2.3\">Evaluation of an ML algorithm using Cross-Validation</a> \n",
    "        * 5.2.4 <a href=\"#5.2.4\">Fine tune the ML model</a> \n",
    "    * 5.3 <a href=\"#5.3\">Deep learning (DL) approach</a> \n",
    "        * 5.3.1 <a href=\"#5.3.1\">Time-Series prediction with Long Short Term Memory (LSTM)</a> \n",
    "            * Data preparation\n",
    "            * Definiton of the DL model architecture: Construction of the neural network\n",
    "            * Compilation of the DL model\n",
    "            * Training of the DL model: Fit model on training data\n",
    "            * Evaluation of the DL model\n",
    "> * 6. <a href=\"#6\">Exercise 2: Building a deep learning model (neural network) with keras</a> \n",
    "        * 6.1 <a href=\"#6.1\">Following the steps in section 5, visit the Keras API and select an architecture for your neural network (e.g. a CNN). Build, train and validate your model. </a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses some of the most important libraries that currently exist in the world of data analysis and Machine Learning (ML). We will work with a dataset on the subject of Internet of Things. The dataset includes information collected by different sensors in different climatic conditions. To show all the possibilities offered by the aforementioned libraries, all the stages that a machine learning project usually has, will be covered. That is to say, data import, data processing and exploration, ML / DL algorithm training and algorithm validation.\n",
    "\n",
    "In this notebook two differentiated parts are distinguished. The first part deals with exploring the dataset to discover its main characteristics. The second part is related to the training and validation of machine learning models using two of the most used libraries (Scikit-Learn and Keras). First, examples are shown to show what can be done with these libraries. In this way, such practical examples serve as an aid to carry out the exercises at the end of each section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Exploratory data analysis<a name=\"Section1\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5OrdvXtCNY5w"
   },
   "source": [
    "## 1. Setup - Libraries import <a name=\"1.\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GydRZ-31NY5x"
   },
   "source": [
    "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zu1JvozANY5z"
   },
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score, median_absolute_error, mean_absolute_error\n",
    "from sklearn.metrics import median_absolute_error, mean_squared_error, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estimators\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Projects\\AIN_Ubiquitous_Computing\\exercise\\sheet 4\\Smart Home Dataset – End-to-end Machine Learning project.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projects/AIN_Ubiquitous_Computing/exercise/sheet%204/Smart%20Home%20Dataset%20%E2%80%93%20End-to-end%20Machine%20Learning%20project.ipynb#Y410sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Common imports\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Projects/AIN_Ubiquitous_Computing/exercise/sheet%204/Smart%20Home%20Dataset%20%E2%80%93%20End-to-end%20Machine%20Learning%20project.ipynb#Y410sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projects/AIN_Ubiquitous_Computing/exercise/sheet%204/Smart%20Home%20Dataset%20%E2%80%93%20End-to-end%20Machine%20Learning%20project.ipynb#Y410sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Projects/AIN_Ubiquitous_Computing/exercise/sheet%204/Smart%20Home%20Dataset%20%E2%80%93%20End-to-end%20Machine%20Learning%20project.ipynb#Y410sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with dates\n",
    "from datetime import datetime\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries for Deep Learning\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Where to save the figures\n",
    "#PROJECT_ROOT_DIR = \".\"\n",
    "#CHAPTER_ID = \"end_to_end_project\"\n",
    "#IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "#os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "#def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "#    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "#    print(\"Saving figure\", fig_id)\n",
    "#    if tight_layout:\n",
    "#        plt.tight_layout()\n",
    "#    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Library information <a name=\"LibInf\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 align=\"center\">Manipulation of data structures and data analysis</h3>\n",
    "<h4 align=\"center\"><a href=\"https://numpy.org/\">Numpy</a></h4>\n",
    "NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more\n",
    "\n",
    "<h4 align=\"center\"><a href=\"https://docs.python.org/3/library/datetime.html\">Datetime and time</a></h4>\n",
    "The datetime module supplies classes for manipulating dates and times. While date and time arithmetic is supported, the focus of the implementation is on efficient attribute extraction for output formatting and manipulation.\n",
    "\n",
    "<h4 align=\"center\"><a href=\"https://pandas.pydata.org/\">Pandas</a></h4>\n",
    "Pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "<h3 align=\"center\">Visualization</h3>\n",
    "<h4 align=\"center\"><a href=\"https://matplotlib.org/\">Matplotlib</a></h4>\n",
    "Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
    "<h4 align=\"center\"><a href=\"https://seaborn.pydata.org/index.html\">Seaborn</a></h4>\n",
    "Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "\n",
    "<h3 align=\"center\">Libraries for Machine and Deep Learning</h3>\n",
    "<h4 align=\"center\"><a href=\"https://scikit-learn.org/stable/\">Scikit-Learn</a></h4>\n",
    "Scikit-learn is an open source machine learning library for Python (built on NumPy, SciPy, and Matplotlib) that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities.\n",
    "\n",
    "<h4 align=\"center\"><a href=\"https://www.tensorflow.org/?hl=es-419\">Tensorflow</a></h4>\n",
    "TensorFlow is an end-to-end open source platform for machine learning. It has a comprehensive, flexible ecosystem of tools, libraries and community resources that lets researchers push the state-of-the-art in ML and developers easily build and deploy ML powered applications.\n",
    "\n",
    "<h4 align=\"center\"><a href=\"https://keras.io/\">Keras</a></h4>\n",
    "Keras is a deep learning API written in Python, running on top of the machine learning platform TensorFlow. It was developed with a focus on enabling fast experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt8e6APDNY52"
   },
   "source": [
    "## 2. Data collection <a name=\"2.\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running a notebook locally, it is very easy to load a dataset using the pandas library. However, when using a notebook on Google Colab, there are several options. In this notebook we will use one of them: Accessing Google Drive by mounting it locally.\n",
    "\n",
    "We use this option to avoid the time we would have to wait if we were to upload the dataset directly from our local storage. See other options here: https://towardsdatascience.com/7-ways-to-load-external-data-into-google-colab-7ba73e7d5fc7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Accessing Google Drive by mounting it locally <a name=\"2.2\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the drive module from google.colab to mount your Google Drive to Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing the above statement, you will be provided an authentication link and a text box to enter your authorization code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click the authentication link and follow the steps to generate your authorization code. Copy the code displayed and paste it into the text box as shown above. Once it is mounted, you should get a message like:\n",
    "\n",
    "*Mounted at /content/drive*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, you should be able to explore the contents via “Files explorer” and read the data as you would normally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, to unmount your Google Drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the dataset available in the google colab environment, we can import and load it in order to work with it. Next we create a function that reads the dataset using the pandas library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change \"FOLDER_NAME\" to your folder name where you saved the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_smarthome_data_drive(data='../sheet 4/smarthome_data.csv'):\n",
    "    dataset = pd.read_csv(data)\n",
    "    print(\"Number of instances: {}\".format(dataset.shape[0])) ##Displays the number of instances of the dataset\n",
    "    print(\"Number of columns: {}\".format(dataset.shape[1])) ##Displays the number of columns of the dataset\n",
    "    return dataset\n",
    "\n",
    "print(\"Loading smarthome dataset...\")\n",
    "dataset = load_smarthome_data_drive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Columns Descriptions:\n",
    "(source: Data Source: https://www.kaggle.com/taranvee/smart-home-dataset-with-weather-information)\n",
    "\n",
    "\n",
    "Below you can see an explanation of each attribute of the dataset.\n",
    "\n",
    "##### Index<br>\n",
    "- **time**: Time of the readings, with a time span of 1 minute.<br>\n",
    "\n",
    "##### Energy Usage<br>\n",
    "- **use [kW]**: Total energy consumption.<br>\n",
    "- **gen [kW]**: Total energy generated by means of solar or other power generation resources.<br>\n",
    "- **House overall [kW]**: overall house energy consumption.<br>\n",
    "- **Dishwasher [kW]**: energy consumed by specific appliance.<br>\n",
    "- **Furnace 1 [kW]**: energy consumed by specific appliance.<br>\n",
    "- **Furnace 2 [kW]**: energy consumed by specific appliance.<br>\n",
    "- **Home office [kW]**: energy consumed by specific appliance.<br>\n",
    "- **Fridge [kW]**: energy consumed by specific appliance.<br>\n",
    "- **Wine cellar [kW]**: energy consumed by specific appliance.<br>\n",
    "- **Garage door [kW]**: energy consumed by specific appliance.<br>\n",
    "- **Kitchen 12 [kW]**: energy consumption in kitchen 1.<br>\n",
    "- **Kitchen 14 [kW]**: energy consumption in kitchen 2.<br>\n",
    "- **Kitchen 38 [kW]**: energy consumption in kitchen 3.<br>\n",
    "- **Barn [kW]**: energy consumed by specific appliance.<br>\n",
    "- **Well [kW]**: energy consumed by specific appliance.<br>\n",
    "- **Microwave [kW]**:energy consumed by specific appliance.<br>\n",
    "- **Living room [kW]**: energy consumption in Living room.<br>\n",
    "- **Solar [kW]**: Solar power generation.<br>\n",
    "\n",
    "##### Weather<br>\n",
    "- **temperature**:Temperature is a physical quantity expressing hot and cold.<br>\n",
    "- **humidity**: Humidity is the concentration of water vapour present in air.<br>\n",
    "- **visibility**:Visibility sensors measure the meteorological optical range which is defined as the length of atmosphere over\n",
    "    which a beam of light travels before its luminous flux is reduced to 5% of its original value.<br>\n",
    "- **apparentTemperature**:Apparent temperature is the temperature equivalent perceived by humans, caused by the combined effects of air temperature, relative humidity and wind speed. The measure is most commonly applied to the perceived outdoor\n",
    "    temperature.<br>\n",
    "- **pressure**:Falling air pressure indicates that bad weather is coming, while rising air pressure indicates good weather.\n",
    "- **windSpeed**:Wind speed, or wind flow speed, is a fundamental atmospheric quantity caused by air moving from high to low\n",
    "    pressure, usually due to changes in temperature.<br>\n",
    "- **cloudCover**:Cloud cover (also known as cloudiness, cloudage, or cloud amount) refers to the fraction of the sky obscured by clouds when observed from a particular location. Okta is the usual unit of measurement of the cloud cover.<br>\n",
    "- **windBearing**: In meteorology, an azimuth of 000° is used only when no wind is blowing, while 360° means the wind is from the North. True Wind Direction True North is represented on a globe as the North Pole. All directions relative to True North may be called \"true bearings.\"<br>\n",
    "- **dewPoint**:the atmospheric temperature (varying according to pressure and humidity) below which water droplets begin to\n",
    "    condense and dew can form.<br>\n",
    "- **precipProbability**: A probability of precipitation (POP), also referred to as chance of precipitation or chance of rain, is\n",
    "    a measure of the probability that at least some minimum quantity of precipitation will occur within a specified forecast\n",
    "    period and location.<br>\n",
    "- **precipIntensity**: The intensity of rainfall is a measure of the amount of rain that falls over time. The intensity of rain is measured in the height of the water layer covering the ground in a period of time. It means that if the rain stays where it falls, it would form a layer of a certain height.<br>\n",
    "\n",
    "##### Others<br>\n",
    "- **summary**:Report generated by the by the data collection systm (apparently!).<br>\n",
    "    * Including: Clear, Mostly Cloudy, Overcast, Partly Cloudy, Drizzle,Light Rain, Rain, Light Snow, Flurries, Breezy, Snow, Rain and Breezy, Foggy, Breezy and Mostly Cloudy, Breezy and Partly Cloudy, Flurries and Breezy, Dry, Heavy, Snow.<br>\n",
    "- **icon**: The icon that is used by the data collection systm (apparently!).<br>\n",
    "    * Including: cloudy, clear-night, partly-cloudy-night, clear-day, partly-cloudy-day, rain, snow, wind, fog.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Discovering and visualizing the data to gain insight <a name=\"3.\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset has been imported, it is the turn of data exploration and visualisation to gain knowledge. It is highly recommended to review all columns and be familiar with the features before moving on to further steps such as feature engineering or algorithm training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Preprocessing <a name=\"3.1\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Data exploration <a name=\"3.1.1\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first section, we use some of the methods provided by the pandas library to perform a first exploratory analysis of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZh6-PZcNY5-"
   },
   "source": [
    "#### Quick Look at the Data Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make use of the methods \"head\" and \"tail\" which return the first 'n' columns and the final 'n' columns of the dataset \n",
    "respectively.\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJDZb_5CNY5_",
    "outputId": "1b7b14bd-380e-43bb-daca-0e8cab60c431"
   },
   "outputs": [],
   "source": [
    "smarthome = load_smarthome_data_drive()\n",
    "print(\"First five rows: \")\n",
    "smarthome.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last five rows: \")\n",
    "smarthome.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many and what type of attributes do we have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we make use of the \"info\" method provided by pandas which prints information about the DataFrame including the index dtype and columns, non-null values and memory usage.\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfy2YlqcNY6B",
    "outputId": "7279cfa6-b0eb-407f-cad3-3a05795691ed"
   },
   "outputs": [],
   "source": [
    "smarthome.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, almost all attributes are of type float except four of them: time (timestamp), icon, summary and cloud cover. All of them are object type, which means that they can contain any type of Python object. But as you have loaded this data from a CSV file, it must be a text attribute. In the following, we look at three of these categorical attributes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding out the categories within the object-type data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave out of this analysis the attribute known as \"time\" as we know that it contains timestamps and this column will be manipulated later in this notebook. Therefore we make use of the value_counts() method to find out how many categories exist in the remaining three attributes, icon, summary and cloudCover. \n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CH7JTtBjNY6C",
    "outputId": "b6d6f213-37ec-4df7-81a7-e2d5919f53d9"
   },
   "outputs": [],
   "source": [
    "smarthome[\"icon\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smarthome[\"summary\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smarthome[\"cloudCover\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, cloudCover does not contain text data. As said, python object can be any kind of data type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of each nummerical attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having looked at the categorical attributes, we now turn to the numerical attributes. Again, the pandas library provides us with the describe() method that displays a summary of all numeric attributes. \n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BaGtVe0NY6D",
    "outputId": "1a25a1ec-b9c5-47c7-bef3-fd462e2af7d4"
   },
   "outputs": [],
   "source": [
    "smarthome.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count, mean, min, and max rows are self-explanatory. Note that the null values are ignored. The std row shows the standard deviation, which measures how dispersed the values are. The 25%, 50%, and 75% rows show the percentiles: a percentile indicates the value below which a given percentage of observations in a group of observations fall. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram for each numerical attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick way to get a fell of the type of data you are dealing with is to plot a histogram for each numerical attribute. A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis). You can either plot this one attribute at a time, or you can call the hist() method on the whole dataset, and it will plot a histogram for each numerical attribute. \n",
    "\n",
    "It is important to note that the *hist()* method relies on Matplotlib, which in turn relies on a user-specified graphical backend to draw on your screen. So before you can plot anything, you need to specify which backend Matplotlib should use. The simplest option is to use Jupyter's magic comand %matplolib inline (we did it in point 1). This tells Jupyter to set up Matplolib so it uses Jupyter's own backend. Plots are then rendered within the notebook itself. Note that calling show() is optional in a Jupyter notebook, as Jupyter will automatically display plots when a cell is executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yn717C4bNY6E",
    "outputId": "45c3666d-48e1-4e2a-96a9-62845a6881ae"
   },
   "outputs": [],
   "source": [
    "smarthome.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What strikes you about these histograms? Look at the histograms with the temperature data - is this correct and does it refer to degrees Celsius? Was the data collected in a rainy place?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Data cleansing <a name=\"3.1.2\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on with our exploratory analysis, it is necessary to take a look at the number of missing features our dataset contains. This is a crucial point in any data analysis, as most Machine Learning algorithms do not work with missing values. There are several options when missing values are found:\n",
    "\n",
    "* Get rid of the instances with NaNs\n",
    "* Get rid of the whole attribute\n",
    "* Set the values to some vaule (zero, the mean, the median, etc. )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching for NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to know how many missing values there are in each attribute.To do so, we call the isnull() method along with the sum() method to add up all the missing values.\n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.isnull.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smarthome.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section 3.1.1 when we took a quick look at the structure of the dataset, we discovered that the last row was composed of NaNs values. So we took a look at that instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smarthome.iloc[-1 ,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, as it is only one row, and the whole row is full of missing values, we can delete it. To do this, the pandas library provides us with the *dropna()* method that eliminates all the rows where there are missing values. This however should not always be used, because even if there is only one attribute with missing values, the row is completely deleted when this method is called. Therefore, if more missing values are found, we would have to opt for some of the techniques described at the beginning of this section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smarthome_cleaned = smarthome.dropna() ## We delete the rows with NaNs and create a new dataset with no missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check again that we have no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smarthome_cleaned.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Feature engineering <a name=\"3.1.3\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section deals with one of the most relevant aspects when it comes to data analysis: feature engineering. Depending on the information we want to extract from the dataset and the machine learning algorithm we want to use, the attributes must undergo different transformations. In our case, this dataset contains time series, however we do not have the date as such, but the timestamp. This is one of the transformations that the dataset must undergo, going from timestamp to dates in standard format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Working with Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data publisher says the dataset contains the readings with a time span of 1 minute of house appliances in kW from a smart meter and weather conditions of that particular region. So, we set freq='min' and convert Uinx time to readable date. We would like to convert this large number that represents a unix timestamp (i.e. \"1284101485\") to a readable date. So, one idea is to know when the start time is. \n",
    "\n",
    "For this section we use the datatime and time libraries provided by Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' start ' , time.strftime('%Y-%m-%d %H:%M:%S %p', time.localtime(int(smarthome_cleaned['time'].iloc[0]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we now know that the start date is 2016-01-01 06:00:00 AM, we can generate a dataset with the dates for all instances with a frequency of 1 min. For this we make use of the *pandas.date_range* function which returns a fixed frequency DatetimeIndex. \n",
    "\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.date_range.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_min = pd.date_range('2016-01-01 06:00 AM', periods=len(smarthome_cleaned),  freq='min') ## Fixed frequency DatetimeIndex.\n",
    "                                                                                            ## datatime type.\n",
    "smarthome_iot = smarthome_cleaned.set_index(time_min.strftime('%Y-%m-%d %H:%M:%S %p')) ## We create a series of string data and \n",
    "                                                                                       ## set it as a index.\n",
    "smarthome_iot['Time'] = time_min ## We create a column in the dataset with the dates apart from the index.\n",
    "smarthome_iot = smarthome_iot.drop(['time'], axis=1) ## We delete the previous column with the timestamps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to datatime type we can easily extract days, months, years from the full date. This can be useful for different visualisations that we will use later on. We extract the days as numbers and the months as strings and add them as two new attributes to the dataset. \n",
    "\n",
    "To do this we store the values in Series (a dataframe in pandas is composed of Series, in short, the columns or attributes of the dataset). In the following code, *for loop* is used to extract the dates from the column with the dates of the dataset ('Time'). Then, once the dates are extracted, the days corresponding to that date are stored in a list. The same happens for the months. \n",
    "\n",
    "The operation is the same as a typical *for loop* written in other languages such as java, but python provides what is known as \"List Comprehensions\" which allows you to write everything in one line as you can see below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smarthome_iot['Day'] = pd.Series([date.day for date in smarthome_iot['Time']],index=time_min.strftime('%Y-%m-%d %H:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smarthome_iot['Month'] = pd.Series([date.strftime(\"%B\") for date in smarthome_iot['Time']],index=time_min.strftime('%Y-%m-%d %H:%M:%S %p'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that the dataset has been modified and new columns with days and months have been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smarthome_iot.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZkd82ZnNY6X"
   },
   "source": [
    "#### Looking for Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search for correlations is another important technique in data analysis. By applying it, we can see how closely related some attributes are to others. This would benefit us in the case of being able to do without attributes that are highly related, or to find out if there are correlations between different events. It is pretty simple to compute the standard correlation coefficient between every pair of attribute using the corr() method. There are different types of correlation coefficient, in this case we use the one known as a *spearman*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4zzz1TINY6X"
   },
   "outputs": [],
   "source": [
    "corr_matrix = smarthome_iot.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how much each attribute correlates with House overall [kW]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdF3gbSENY6X",
    "outputId": "ecb4a130-1132-4881-ecff-17d06361b119"
   },
   "outputs": [],
   "source": [
    "corr_matrix[\"House overall [kW]\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient ranges from -1 to 1. When it is close to 1, it means that there is a moderate positive correlation; for example, the *House overall [kW]* tends to go up when the *Kitchen 14 [kW]* goes up. When the coefficient is close to -1, it means that there is a strong negative correlation. Finally, coefficients close to 0 mean that there is no linear correlation. In this case there is a moderate negative correlation between the *Solar[kW] or gen[kW]* and *House overall [kW]*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to check for correlation between attributes is to use the pandas scatter_matrix() function, which plots every numerical attribute against every other numercial attribute. Since there are now 28 numerical attributes, you would get 28x28 = 784 plots, which would not fit on a page. So let's focus on a few promising attributes that seem most correlated with the *House overall [kW]*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hJL0bbuNY6Y",
    "outputId": "6cd0f1e5-6a0c-44dd-b267-84a3b985a989"
   },
   "outputs": [],
   "source": [
    "# from pandas.tools.plotting import scatter_matrix # For older versions of Pandas\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "attributes = [\"House overall [kW]\",\"Kitchen 14 [kW]\",'Furnace 1 [kW]']\n",
    "scatter_matrix(smarthome_iot[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Visualization / Outputs <a name=\"3.1.4\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation in data analysis is not only important to gain insight into the data you are working with, but also to reinforce the answers to questions that have arisen prior to the data analysis process. When working with Python for data analysis, the options for good visualisations are many. Matplotlib is usually the epicentre of it all, although there are other solutions that rely on matplotlib to make some of the tasks simpler and get more eye-catching graphs, one example is the Seaborn library.\n",
    "\n",
    "https://seaborn.pydata.org/index.html\n",
    "\n",
    "This library has a gallery where you can see all the graphics available. It is as simple as visiting the gallery and selecting the one you want. Afterwards, you only have to implement small changes in the code. \n",
    "\n",
    "You can also take a look at the Python Graph Gallery where there are a large number of solutions for visualisations in Python.\n",
    "\n",
    "https://www.python-graph-gallery.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### House overall [kW] per month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we look at how consumption has evolved over the months. To do so, we make use of the Seaborn library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "\n",
    "sm_kw_overall = smarthome_iot[[\"Time\",\"Day\",\"Month\",\"House overall [kW]\"]] ## We create a subset from the datset \n",
    "                                                                            ## with the columns we want to study\n",
    "\n",
    "sns.lineplot(data=sm_kw_overall,x=\"Month\", y=\"House overall [kW]\", linewidth=2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exercise 1: Exploratory data analysis <a name=\"4.\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer the exercises try to make some visualisation using some of the libraries described. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Visualise consumption by season of the year <a name=\"4.1\">\n",
    "* **Winter**: December to March\n",
    "* **Spring**: March to June\n",
    "* **Summer**: June to September\n",
    "* **Autumn**: September to December"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set(rc = {'figure.figsize':(8,4)})\n",
    "sm_kw_overall = smarthome_iot[[\"Month\",\"House overall [kW]\"]] ## We create a subset from the datset with the columns we want to study\n",
    "sm_kw_winter = sm_kw_overall[sm_kw_overall['Month'].isin(['December','January','February'])]\n",
    "sm_kw_spring = sm_kw_overall[sm_kw_overall['Month'].isin(['March','April','May'])]\n",
    "sm_kw_summer = sm_kw_overall[sm_kw_overall['Month'].isin(['June','Juli','August'])]\n",
    "sm_kw_autumn = sm_kw_overall[sm_kw_overall['Month'].isin(['September','October','November'])]\n",
    "#plot all four diagrams in one figure\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "sns.lineplot(ax=axes[0, 0],data=sm_kw_winter,x=\"Month\", y=\"House overall [kW]\", linewidth=2.5)\n",
    "sns.lineplot(ax=axes[0, 1],data=sm_kw_spring,x=\"Month\", y=\"House overall [kW]\", linewidth=2.5)\n",
    "sns.lineplot(ax=axes[1, 0],data=sm_kw_summer,x=\"Month\", y=\"House overall [kW]\", linewidth=2.5)\n",
    "sns.lineplot(ax=axes[1, 1],data=sm_kw_autumn,x=\"Month\", y=\"House overall [kW]\", linewidth=2.5)\n",
    "axes[0, 0].set_title('Winter')\n",
    "axes[0, 1].set_title('Spring')\n",
    "axes[1, 0].set_title('Summer')\n",
    "axes[1, 1].set_title('Autumn')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 In which month within each season is the most energy consumed? <a name=\"4.1.1\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In which month within each season is the most energy consumed?\n",
    "top_winter = sm_kw_winter.groupby(['Month']).sum().sort_values(by='House overall [kW]', ascending=False).head(1)\n",
    "top_spring = sm_kw_spring.groupby(['Month']).sum().sort_values(by='House overall [kW]', ascending=False).head(1)\n",
    "top_summer = sm_kw_summer.groupby(['Month']).sum().sort_values(by='House overall [kW]', ascending=False).head(1)\n",
    "top_autumn = sm_kw_autumn.groupby(['Month']).sum().sort_values(by='House overall [kW]', ascending=False).head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Which appliances consume the most energy within that period? <a name=\"4.1.2\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Is there any relationship between weather conditions and the highest energy consumption in those peak periods? <a name=\"4.1.3\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 In which weather conditions was the most energy consumed? <a name=\"4.1.4\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Machine Learning and Deep Learning models <a name=\"Section2\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data analysis - Different approaches <a name=\"5.\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the exploratory analysis has been carried out, it is time to move on to the selection of models for prediction or classification. In this section we will go from solutions that do not implement any ML algorithm to Deep Learning, including the use of classical ML algorithms. As our dataset contains time series, some of the methods / algorithms that are commonly used with this type of data are shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Non-Machine Learning approach <a name=\"5.1\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving Average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, a moving average is a calculation used to analyze data points by creating a series of averages of different subsets of the full data set. In finance, a moving average (MA) is a stock indicator that is commonly used in technical analysis. The reason for calculating the moving average of a stock is to help smooth out the price data by creating a constantly updated average price. In short, it helps us to visualise the trend of the time series.\n",
    "\n",
    "It basically states that the next observation is the mean of every past observation. Therefore, we assume that the future value of our variable depends on the average of its  *k*  previous values. \n",
    "\n",
    "$$\n",
    "y_t = 1/k \\sum (y_t - n )\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has an implementation available with *DataFrame.rolling(window).mean()* \n",
    "\n",
    "In the following code, the moving average model is implemented within a function that shows us in a visual way the result after its application (i.e. the trend). The function also includes the option to display the anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def plotMovingAverage(series, window, plot_intervals=False, scale=1.96, plot_anomalies=False):\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "        series - dataframe with timeseries\n",
    "        window - rolling window size \n",
    "        plot_intervals - show confidence intervals\n",
    "        plot_anomalies - show anomalies \n",
    "    \"\"\"\n",
    "    \n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "\n",
    "    plt.figure(figsize=(25,5))\n",
    "    plt.title(\"Moving average with window size = {}\".format(window))\n",
    "    plt.plot(rolling_mean, \"g\", label=\"Rolling mean trend\")\n",
    "\n",
    "    # Plot confidence intervals for smoothed values\n",
    "    if plot_intervals:\n",
    "        mae = mean_absolute_error(series[window:], rolling_mean[window:])\n",
    "        deviation = np.std(series[window:] - rolling_mean[window:])\n",
    "        lower_bond = rolling_mean - (mae + scale * deviation)\n",
    "        upper_bond = rolling_mean + (mae + scale * deviation)\n",
    "        plt.plot(upper_bond, \"r--\", label=\"Upper Bond / Lower Bond\")\n",
    "        plt.plot(lower_bond, \"r--\")\n",
    "        \n",
    "        # Having the intervals, find abnormal values\n",
    "        if plot_anomalies:\n",
    "            anomalies = pd.DataFrame(index=series.index, columns=series.columns)\n",
    "            anomalies[series<lower_bond] = series[series<lower_bond]\n",
    "            anomalies[series>upper_bond] = series[series>upper_bond]\n",
    "            plt.plot(anomalies, \"ro\", markersize=10)\n",
    "        \n",
    "    plt.plot(series[window:], label=\"Actual values\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wider the window, the smoother the trend. In the case of very noisy data, which is often encountered in finance, this procedure can help detect common patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving Average with a window of 6 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 24*30 # 1 month\n",
    "cols = ['House overall [kW]']\n",
    "plotMovingAverage(smarthome_cleaned[cols][:n_samples], window=6) # A window of 6 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Moving Average with a window of 24 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMovingAverage(smarthome_cleaned[cols][:n_samples], window=24) # A window of 24 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to detect anomaly in time-series is using the moving average as the trend of the data and points that are deviate from the moving average be considered as anomaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotMovingAverage(smarthome_cleaned[cols][:n_samples], window=24, plot_intervals=True, plot_anomalies=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Machine learning approach <a name=\"5.2\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we are going to train several estimators to predict what the energy consumption will be, using as input data, the data that refer to the climatic conditions.Training a regressor is very simple in Sklearn. As with a classifier, just create an object of the estimator you want to train and execute the <code>fit</code> method. In this notebook we use two of the most common regressors: <code>LinearRegression</code> and <code>Random Forest Regressor</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Simple ML Algorithm implementation using Scikit-Learn <a name=\"5.2.1\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisation is a technique often applied as part of data preparation for machine learning.  The purpose of normalisation is to change the values in the numerical columns of the data set to use a common scale, without distorting the differences in the ranges of values or losing information. Normalisation is also necessary for some algorithms to model the data correctly, such as Support vector machine (SVM). In our case for Linear Regression and Random Forest Regressor it is not essential for the data to be normalised, so we will not implement it. \n",
    "\n",
    "In this data preparation stage, we split the data into training and tes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_for_training_filtered_by_weather(dataset):\n",
    "    \n",
    "    X = dataset[['temperature','humidity', 'visibility','windSpeed', 'pressure', 'windBearing', 'precipIntensity','precipProbability']]\n",
    "    Y = dataset['use [kW]']\n",
    "    \n",
    "    train_ratio = 0.80\n",
    "    test_ratio = 0.20\n",
    "\n",
    "    # Using train_test_split() from the data science library scikit-learn, you can split your dataset into subsets \n",
    "    # that minimize the potential for bias in your evaluation and validation process\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_ratio)\n",
    "    \n",
    "    \n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = prepare_dataset_for_training_filtered_by_weather(smarthome_iot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression is a mathematical model used to approximate the relationship between a dependent variable $y$, and the independent variables $x_i$. The model is expressed by the following formula:\n",
    "\n",
    "$$\n",
    "y \\approx \\alpha+\\beta_1x_1+\\beta_2x_2...+\\beta_nx_n\n",
    "$$\n",
    "\n",
    "Sklearn provides several methods to perform linear regression. The simplest of these is the _least squares_ method, which is implemented by the <code>LinearRegression</code> estimator. The least squares technique is used to determine the coefficients of a regression function that minimise the sum of squares of the errors. For a linear regression function, one would try to minimise this expression:\n",
    "\n",
    "$$\n",
    "S = \\sum (y - f(X))^2 = \\sum (y - \\alpha+\\beta_1x_1+\\beta_2x_2...+\\beta_nx_n)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We imported the LinearRegresssion model in section *1. Setup - Libraries import*. It is time to create the estimator and train it on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearRegressor = LinearRegression() \n",
    "linearRegressor.fit(x_train,y_train) ## Fit linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a regressor is trained, we can use it to predict the class of a set of instances with the <code>predict</code> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearRegressor.predict(x_train[:5]) ## Here we select the first 5 instances of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the coefficient of determination of the prediction.The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a  R<sup>2</sup> score of 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linearRegressor.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests (RF) construct many individual decision trees at training. Predictions from all trees are pooled to make the final prediction; the mode of the classes for classification or the mean prediction for regression. As they use a collection of results to make a final decision, they are referred to as Ensemble techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestRss = RandomForestRegressor(max_depth=5, random_state=0)\n",
    "RandomForestRss.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a regressor is trained, we can use it to predict the class of a set of instances with the <code>predict</code> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestRss.predict(x_train[:5]) ## Here we select the first 5 instances of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the coefficient of determination of the prediction.The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a  R<sup>2</sup> score of 0.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestRss.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Evaluation of an ML algorithm (Regression) <a name=\"5.2.2\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In classification tasks the evaluation metrics are based on the number of correct predictions. In regression, however, one cannot speak of accuracy since the predictions are numerical and it is very unlikely to predict exactly the correct value. The important thing in evaluating a regressor is to measure the difference between the actual value and the predicted value.\n",
    "\n",
    "Three of the most popular metrics for assessing the quality of regressors:\n",
    "\n",
    "**Mean Absolute Error**\n",
    "$$\n",
    "MAE = \\frac{\\sum |\\;y -f(X)\\;|}{n}\n",
    "$$\n",
    "**Mean Squared Error**\n",
    "$$\n",
    "MSE = \\frac{\\sum (y -f(X))^2}{n}\n",
    "$$\n",
    "**Coefficient of determination**\n",
    "$$\n",
    "R2 = 1 - \\frac{\\sum (y -f(X))^2}{\\sum (\\bar{y} - y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Evaluation of an ML algorithm using Cross-Validation <a name=\"5.2.3\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to evaluate the models would be to use the *train_test_split()* function to split the training set into a smaller training set and a validation set, then train the models against the smaller training set and evaluate them against the validation set. It works fairly well. However, a great alternative is to use Scikit-Learn's K-fold cross-validation feature. In this section, we are going to split the training set into 10 distinct subset called folds, then it trains and evaluate the models 10 times, picking a different fold for evaluaiton every time and training on the other 9 folds. The result will be an array containing the 10 evaluation scores. \n",
    "\n",
    "We are going to use a function that display the result after the training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean:\", scores.mean())\n",
    "    print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_linear_regression = cross_val_score(linearRegressor, x_train, y_train, scoring=\"neg_mean_squared_error\", cv=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(np.sqrt(-scores_linear_regression))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cross-validation predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(linearRegressor,x_train,y_train,cv=20)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cross-validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_random_forest = cross_val_score(RandomForestRss, x_train, y_train, scoring=\"neg_mean_squared_error\", cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_scores(np.sqrt(-scores_random_forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cross-validation predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = cross_val_predict(linearRegressor,x_train,y_train,cv=3)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.4 Fine tune the ML model <a name=\"5.2.4\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest regressor yiels better results than Linear Regression. Once we have decided what models is best, it is time to fine-tune it. One option would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values. This would be tedious work, and you may not have time to explore many combinations.\n",
    "\n",
    "Instead, we are going to use *Scikit-Learn's GridSearchCV* to search. All we need to do is to select a set of hyperparmeters to experiment and what values to try out. Then it will use corss-validation to evaluate all the possible vominations of hyperparmer values. For example, the following code searches for the best combination of hyperparameter values for the RandomForestRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9-a5SakNY7C",
    "outputId": "7f9a6f7e-057a-497d-a701-6c135a0e2060"
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    # try 9 (3×3) combinations of hyperparameters\n",
    "    {'n_estimators': [5, 10, 20],'max_features': [2, 4, 6]},\n",
    "    # then try 4 (2×2) combinations with bootstrap set as False\n",
    "    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 4]},\n",
    "  ]\n",
    "\n",
    "RandomForestRss_Grid = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# train across 2 folds, that's a total of (9+4)*2= 26 rounds of training \n",
    "grid_search = GridSearchCV(RandomForestRss_Grid, param_grid, cv=2,\n",
    "                           scoring='neg_mean_squared_error',\n",
    "                           return_train_score=True)\n",
    "\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have no idea what value a hyperparameter should have, a simple approach is to try out consecutive powers of 10 (or a smaller number if you want a more fine-grained search). \n",
    "\n",
    "The *param_grid* tells Scikit-Learn to first evaluate all 3 x 3 = 9 combinations of n_estimators and max_features hyperparmeter values specified in the first dictionary (dict). Then try all 2 x 2 = 4 combinations of hyperparmeter values in the second dict, but this time with the boostrap hyperparameter set to False. \n",
    "\n",
    "The grid search will explore 9 + 4 = 13 combinations of RandomForestRegressor hyperparameter values, and it will train each model 2 times, since we are using two-fold cross validation. (cv should be higher, but the more high is the cv, the more time it takes to look for the best parameters)\n",
    "\n",
    "It may take a bit of time!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7N09NW1BNY7C"
   },
   "source": [
    "Once it is done, we can get the best hyperparameter combination found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6KbSCiaNY7D",
    "outputId": "a61655f9-554a-45f6-988b-ce1dfebde1ad"
   },
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the best estimator directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RWMudvPPNY7D",
    "outputId": "c15accd6-1221-46f7-de57-5c91e5280271"
   },
   "outputs": [],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5rUCrV3yNY7D"
   },
   "source": [
    "Let's look at the score of each hyperparameter combination tested during the grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joKozzyrNY7E",
    "outputId": "bc37906c-13d8-4fca-ecc2-3ebe06cccdef"
   },
   "outputs": [],
   "source": [
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMK3uv5ONY7E",
    "outputId": "af12099d-95d8-4437-bd88-cc702a448c97"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHeAj5tkNY7H"
   },
   "source": [
    "####  Evaluation of the System on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5r4b8LUNY7H"
   },
   "outputs": [],
   "source": [
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "final_predictions = final_model.predict(x_test)\n",
    "\n",
    "final_mse = mean_squared_error(y_test, final_predictions)\n",
    "final_rmse = np.sqrt(final_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBLqSC1sNY7H",
    "outputId": "acd962ac-8816-4c04-e021-ff7d13def300"
   },
   "outputs": [],
   "source": [
    "final_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the best parameters have been selected and the model has been trained. This stage will be completed and the model will be saved for production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X940ROGVNY7L"
   },
   "source": [
    "#### Model persistence using joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily save Scikit-Learn models by using Python's pickle module or by using the joblib library, which is more efficient at serializing large NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPmOxdQ3NY7M"
   },
   "outputs": [],
   "source": [
    "##import joblib\n",
    "##joblib.dump(my_model, \"my_model.pkl\") # DIFF\n",
    "#...\n",
    "##my_model_loaded = joblib.load(\"my_model.pkl\") # DIFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PIJ7wCHNY7N"
   },
   "source": [
    "### 5.3. Deep learning approach <a name=\"5.3\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will build and train a neural network for the same purpose as machine learning models. Predicting energy consumption depending on weather conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1 Time-Series prediction with LSTM - Deep Learning <a name=\"5.3.1\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise we are going to build and train an LSTM using the keras library. You can see a similar example here: \n",
    "\n",
    "https://keras.io/examples/timeseries/timeseries_weather_forecasting/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need to separate the data into data for training, for testing and in this case that we make use of a Deep Learning model, we also have the validation data. The validation data is used to validate how good the training of the network is before it sees the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_for_training_filtered_by_weather(dataset):\n",
    "    \n",
    "    X = dataset[['temperature','humidity', 'visibility','windSpeed', 'pressure', 'windBearing', 'precipIntensity','precipProbability']]\n",
    "    Y = dataset['use [kW]']\n",
    "    \n",
    "    ## Percentage of dataset for training, test and validation\n",
    "    train_ratio = 0.75\n",
    "    validation_ratio = 0.15\n",
    "    test_ratio = 0.10\n",
    "\n",
    "    # Train is now 75% of the entire data set\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=1 - train_ratio)\n",
    "    \n",
    "    # Test is now 10% of the initial data set\n",
    "    # Validation is now 15% of the initial data set\n",
    "    x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio))\n",
    "    \n",
    "    # The Keras library works with tensors or numpy arrays. \n",
    "    # Therefore we need to transform our input data into tensors that tell us how many instances we have, how many attributes \n",
    "    # and the number of dimensions of the dataset (this is especially important if you work with photographs). \n",
    "    x_train = np.reshape(x_train.values, (x_train.shape[0], x_train.shape[1],1))\n",
    "    x_val = np.reshape(x_val.values, (x_val.shape[0], x_test.shape[1],1))\n",
    "    x_test = np.reshape(x_test.values, (x_test.shape[0], x_test.shape[1],1))\n",
    "    \n",
    "    return x_train,x_val,x_test,y_train,y_val,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,x_test,y_train,y_val,y_test = prepare_dataset_for_training_filtered_by_weather(smarthome_iot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x_train: {}, y_train: {} ; x_test: {}, y_test {} ; x_val: {}, y_val: {}\".format(x_train.shape, y_train.shape, x_test.shape,\n",
    "                                                                                    y_test.shape,x_val.shape,y_val.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definiton of the DL model architecture: Construction of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the model using the Sequential API.\n",
    "\n",
    "* 1) The first line creates a Sequential model. This is the simplest kind of Keras model for neural networks that are jus compose of a single stack of layers connected sequentally. This is called the Sequential API.\n",
    "* 2) Then we add 2 layers, two LSTM layers with 32 neurons each: https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "* 3) Finally we add a Dense output layer with 1 neuron (output): https://keras.io/api/layers/core_layers/dense/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=32, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "model.add(LSTM(units=32))\n",
    "model.add(Dense(units = 1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's summary() method displays all the model's layers, including each layer's name, its output shape and its number of parameters. The summary ends with the total nubmer of parameters, including trainable and non-trainable parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compilation of the DL model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a model is created, you must call ist compile() method to specify the loss function and the optimizer to use. Optionally, you can specify a list of extra metrics to compute during training and evaluation. \n",
    "\n",
    "https://keras.io/api/models/model_training_apis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training of the DL model: Fit model on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model is ready to be trained. For this we simply need to call its fit() method. An epoch means training the neural network with all the training data for one cycle. In an epoch, we use all of the data exactly once. An epoch is made up of one or more batches, where we use a part of the dataset to train the neural network.\n",
    "\n",
    "We pass the model he impot features (x_train) and the target (y_train), as well as the number of epochs to train. We also pass a validation set. Keras will measure the loss and the extra metrics on this set at the end of each epoch, whichis very useful to see how well the model really performs. If the performance on the training set is much better than on the validation set, the model is probably overfitting the training set.\n",
    "\n",
    "At each epoch during training, Keras displays the number of instances processed so far (along with a progress bar), the mean training time per sample, and the loss and the accuracy  or any extra metrics (if asked) on both the training set and validation set. \n",
    "\n",
    "The *fit()* method returns a History object containing the training parameters (history.params), the list of epochs it went through (history.epoch) and most importantly a dictionary (history.history) containing the loss and extra metrics it measured at the end of each epoch on the training set an on the validation set. If you use this dictionary to create pandas DataFrame and call its plot() method, you will get the learning curves shown down below in the *Evaluation of the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs = 10, batch_size = 264, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of the DL model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning curves: The mean training loss measured over each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1.20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are satisfied with the model's validation accuracy  or loss, we should evaluate it on the test set to estimate the generalization error before we deploy the model to production. We can do it easily by calling the evaluate() method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the results to see how good they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "plt.figure(figsize=(25,10))\n",
    "plt.plot(y_test.values[::24], color='blue', label='Original Usage')\n",
    "plt.plot(predictions[:,0][::24] , color='red', label='Predicted Usage')\n",
    "plt.title('Energy Usage Prediction')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('kW')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Exercise 2: Building a deep learning model (neural network) with keras <a name=\"6\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Following the steps in section 5, visit the Keras API and select an architecture for your neural network (e.g. a CNN is known to yield good results when working with time series). Build, train and validate your model. <a name=\"6.1\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "02_end_to_end_machine_learning_project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
